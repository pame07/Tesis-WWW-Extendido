# -*- coding: utf-8 -*-
"""Copia de bot detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PK0XCaE8-_Zrg978rl75wLxjokRBkUAU
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as pyplot
import pickle

# Import the Python machine learning libraries we need
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier 
from sklearn.metrics import classification_report, confusion_matrix,  precision_recall_fscore_support, accuracy_score, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

# Import some convenience functions.  This can be found on the course github
# Load the data set
dataset = pd.read_csv("https://raw.githubusercontent.com/magahernandez/test/main/training_dataset_ENG.csv", engine='python', encoding='latin-1')

test_data = pd.read_csv("https://raw.githubusercontent.com/pame07/datasets/main/test_dataset_ENG.csv", engine='python', encoding='latin-1')

dataset.head()

# Split into input and output features
y = dataset["label"]
X = dataset[["retweet_count","favorite_count","num_hashtags","num_urls","num_mentions","statuses_count","followers_count","friends_count","favourites_count","listed_count","url_ratio","hashtag_ratio","mention_ratio","entropy","wavelet_avg"]]

# Split into test and training sets
test_size = 0.20
seed = 7
X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=test_size, random_state=seed)

#Preparing test dataset for prediction

#lb2 = LabelEncoder()
#lb2.fit(test_data['label'])
#test_data['label'] = lb2.transform(test_data['label'])

y2 = test_data["label"]
X2 = test_data[["retweet_count","favorite_count","num_hashtags","num_urls","num_mentions","statuses_count","followers_count","friends_count","favourites_count","listed_count","url_ratio","hashtag_ratio","mention_ratio","entropy","wavelet_avg"]]

clasificadores=[]
lista_metricas=[]
matrices_cm=[]

#Capturing metrics for testing

clasificadores_Test=[]
lista_metricas_Test=[]
matrices_cm_Test=[]

def display_metrics_Test(clasif, label, prediction, clases=None):
  print(classification_report(label, prediction))
  metricas=precision_recall_fscore_support(label, prediction, average='binary', labels=clases,pos_label='human')
#  acc=accuracy_score(y_test, y_pred)
  metricas = metricas + (accuracy_score(label, prediction),)
  print('precision:', metricas[0], ' recall: ', metricas[1], ' F1: ', metricas[2],' acc: ',  metricas[4])
  cm = confusion_matrix(label, prediction, labels=clases)
  #disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clases)
  #disp.plot(cmap=plt.cm.Blues)
  clasificadores_Test.append(clasif)
  lista_metricas_Test.append(metricas)
  matrices_cm_Test.append(cm)
  #plt.show()

def display_metrics(clasif, y_test, y_pred, clases=None):
  print(classification_report(y_test, y_pred))
  metricas=precision_recall_fscore_support(y_test, y_pred, average='binary', pos_label='human')
#  acc=accuracy_score(y_test, y_pred)
  metricas = metricas + (accuracy_score(y_test, y_pred),)
  print('precision:', metricas[0], ' recall: ', metricas[1], ' F1: ', metricas[2],' acc: ',  metricas[4])
  cm = confusion_matrix(y_test, y_pred, labels=clases)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=clases)
  disp.plot(cmap=plt.cm.Blues)
  clasificadores.append(clasif)
  lista_metricas.append(metricas)
  matrices_cm.append(cm)
  plt.show()

"""# **Decision Tree**"""

# New https://www.kaggle.com/code/valkenberg/twitter-sentiment-analysis-v2

#vectorizer = TfidfVectorizer(analyzer = "word") #Using TFIDF to vectorize the text
#vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,3), max_features = 40000) 
#vectorizer.fit(X_train['clean'].values)   
#x_tr=vectorizer.transform(X_train['clean'].values)
#x_te=vectorizer.transform(X_test['clean'].values)

model1 = DecisionTreeClassifier(
    criterion = "entropy",
    splitter = "best",
    max_depth= 250,
    min_samples_split= 15,
    min_samples_leaf= 30,
    max_features= 0.95
)
print("Training results")
y_pred = model1.fit(X_train, y_train).predict(X_test)
display_metrics('Decision tree', y_test, y_pred, model1.classes_)
print("\nTest results")
test_pred = model1.predict(X2)
display_metrics_Test('Decision tree', y2, test_pred, model1.classes_)

forest_importances = pd.Series(model1.feature_importances_, index=model1.feature_names_in_)

fig, ax = plt.subplots()
std1 = [tree for tree in model1.feature_importances_]
forest_importances.plot.bar(yerr=std1, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

from sklearn import tree
from matplotlib.pyplot import figure

figure(figsize=(15, 15), dpi=300)

tree.plot_tree(model1)
plt.show()

"""# **Random Forest**"""

model2 = RandomForestClassifier(
    n_estimators= 300,
    criterion= "gini",
    max_depth= 200,
    min_samples_split= 5, 
    max_features= 0.55,
    max_leaf_nodes= 15
)
print("Training results")
y_pred = model2.fit(X_train, y_train).predict(X_test)
display_metrics('Random Forest', y_test, y_pred, model2.classes_)
print("\nTest results")
test_pred = model2.predict(X2)
display_metrics_Test('Random Forest', y2, test_pred, model2.classes_)

"""# **Características importantes**
https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py
"""

model2.feature_importances_

model2.feature_names_in_

forest_importances = pd.Series(model2.feature_importances_, index=model2.feature_names_in_)

fig, ax = plt.subplots()
std = np.std([tree.feature_importances_ for tree in model2.estimators_], axis=0)
#forest_importances.plot.bar(yerr=std, ax=ax)
forest_importances.plot.bar(y=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

"""Feature importances are provided by the fitted attribute feature_importances_ and they are computed as the mean and standard deviation of accumulation of the impurity decrease within each tree.


"""

forest_importances = pd.Series(model2.feature_importances_, index=model2.feature_names_in_)

fig, ax = plt.subplots()
std = np.std([tree.feature_importances_ for tree in model2.estimators_], axis=0)
forest_importances.plot.bar(yerr=std, ax=ax)
#forest_importances.plot.bar(y=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()

"""Permutation feature importance overcomes limitations of the impurity-based feature importance: they do not have a bias toward high-cardinality features and can be computed on a left-out test set.

Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled [1]. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature.

[1] L. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
"""

from sklearn.inspection import permutation_importance

#start_time = time.time()
result = permutation_importance(
    model2, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2
)
#elapsed_time = time.time() - start_time
#print(f"Elapsed time to compute the importances: {elapsed_time:.3f} seconds")

forest_importances = pd.Series(result.importances_mean, index=model2.feature_names_in_)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=result.importances_std, ax=ax)
#ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()

"""# **BaggingClassifier**

"""

model3 = BaggingClassifier(
    n_estimators= 50,
    max_samples= 0.45,
    max_features= 0.85 
) 
print("Training results")
y_pred = model3.fit(X_train, y_train).predict(X_test)
display_metrics('Bagging', y_test, y_pred)
print("\nTest results")
test_pred = model3.predict(X2)
display_metrics_Test('Bagging', y2, test_pred, model3.classes_)

"""# **AdaBoostClassifier**"""

model4 = AdaBoostClassifier(
    #base_estimator= RandomForestClassifier(),
    n_estimators= 300,
    learning_rate= 0.85,
    algorithm= "SAMME",
)
print("Training results")
y_pred = model4.fit(X_train, y_train).predict(X_test)
display_metrics('Adaboost', y_test, y_pred)
print("\nTest results")
test_pred = model4.predict(X2)
display_metrics_Test('Adaboost', y2, test_pred, model4.classes_)

"""# **DecisionTreeRegressor**"""

'''model = DecisionTreeRegressor(
    criterion= "mse",
    splitter= "best",
    max_depth= 250,
    max_features= 'log2'
)
display_metrics(y_test, y_pred)'''

"""# **LogisticRegression**"""

model5 = LogisticRegression(
    max_iter= 100000,
    #penalty= "l2",
    #tol= 0.25,
    #C= 0.65,
    #solver= "saga",
    #l1_ratio= 0.68
)
print("Training results")
y_pred = model5.fit(X_train, y_train).predict(X_test)
display_metrics('Logistic Regression', y_test, y_pred)
print("\nTest results")
test_pred = model5.predict(X2)
display_metrics_Test('Logistic Regression', y2, test_pred, model5.classes_)

"""# **Resultados**"""

import seaborn as sns
fig, axn = plt.subplots(1,5, sharex=True, sharey=True,figsize=(12,2))

for i, ax in enumerate(axn.flat):
#    k = list(matrices_cm)[i]
    sns.heatmap(matrices_cm[i], ax=ax,cbar=i==4)
    ax.set_title(clasificadores[i],fontsize=8)

f, axes = plt.subplots(1, 5, figsize=(20, 5), sharey='row')

for i  in range(len(clasificadores)):
    disp = ConfusionMatrixDisplay(matrices_cm[i], display_labels=['Human','Bot'])
    disp.plot(ax=axes[i], xticks_rotation=45, cmap=plt.cm.Blues)
    disp.ax_.set_title(clasificadores[i])
    disp.im_.colorbar.remove()
    disp.ax_.set_xlabel('')
    if i!=0:
        disp.ax_.set_ylabel('')

f.text(0.4, 0.1, 'Predicted label', ha='left')
plt.subplots_adjust(wspace=0.40, hspace=0.1)


f.colorbar(disp.im_, ax=axes, fraction=0.009, pad=0.04)
plt.show()

#Training
print('Classifier', 'Precision', 'Recall', 'F1-score',  'Accuracy')

for i in range(len(clasificadores)):
  print(clasificadores[i],"{:.5f}".format(lista_metricas[i][0]),"{:.5f}".format(lista_metricas[i][1]),"{:.5f} ".format(lista_metricas[i][2]),"&{:.5f} ".format(lista_metricas[i][4]))

"""# Testing Results"""

f, axes = plt.subplots(1, 5, figsize=(20, 5), sharey='row')

for i  in range(len(clasificadores)):
    disp = ConfusionMatrixDisplay(matrices_cm_Test[i], display_labels=['Human','Bot'])
    disp.plot(ax=axes[i], xticks_rotation=45, cmap=plt.cm.Blues)
    disp.ax_.set_title(clasificadores[i])
    disp.im_.colorbar.remove()
    disp.ax_.set_xlabel('')
    if i!=0:
        disp.ax_.set_ylabel('')

f.text(0.4, 0.1, 'Predicted label', ha='left')
plt.subplots_adjust(wspace=0.40, hspace=0.1)


f.colorbar(disp.im_, ax=axes, fraction=0.009, pad=0.04)
plt.show()

#Test
print('Classifier', 'Precision', 'Recall', 'F1-score',  'Accuracy')

for i in range(len(clasificadores)):
  print(clasificadores_Test[i],"{:.5f}".format(lista_metricas_Test[i][0]),"{:.5f}".format(lista_metricas_Test[i][1]),"{:.5f}".format(lista_metricas_Test[i][2]),"{:.5f}".format(lista_metricas_Test[i][4]))

"""# Training v/s Validation Results"""

#Training + Validation
print('Classifier','&', 'Precision', '&','Recall','&', 'F1-score', '&', 'Accuracy')

for i in range(len(clasificadores)):
  print(clasificadores[i],"&","{:.5f}".format(lista_metricas[i][0]),"&","{:.5f}".format(lista_metricas[i][1]),"&","{:.5f}".format(lista_metricas[i][2]),"&","{:.5f}".format(lista_metricas[i][4]),"&","{:.5f}".format(lista_metricas_Test[i][0]),"&","{:.5f}".format(lista_metricas_Test[i][1]),"&","{:.5f}".format(lista_metricas_Test[i][2]),"&","{:.5f}".format(lista_metricas_Test[i][4]))

"""# Integration test

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-pb0m{border-color:inherit;text-align:center;vertical-align:bottom}
.tg .tg-8d8j{text-align:center;vertical-align:bottom}
.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}
.tg .tg-7zrl{text-align:left;vertical-align:bottom}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-pb0m">user</th>
    <th class="tg-pb0m">label</th>
    <th class="tg-pb0m">lang</th>
    <th class="tg-pb0m">Botometer</th>
    <th class="tg-pb0m">Credibility (old)</th>
    <th class="tg-pb0m">Time (old)</th>
    <th class="tg-pb0m">Credibility (new)</th>
    <th class="tg-pb0m">Time (new)</th>
    <th class="tg-pb0m">Detected as</th>
    <th class="tg-pb0m">Verified Weight</th>
    <th class="tg-pb0m">Creation  Weight</th>
    <th class="tg-8d8j">Creation Date</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-pb0m">1GisellePizarro</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">0.6</td>
    <td class="tg-pb0m">36.192919815217394</td>
    <td class="tg-pb0m">1044 ms</td>
    <td class="tg-pb0m">36.192919815217394</td>
    <td class="tg-pb0m">3313 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">34.375</td>
    <td class="tg-7zrl">2011-02-22 04:37:34+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">1Nicoleromany</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">0.2</td>
    <td class="tg-pb0m">48.38181343203563</td>
    <td class="tg-pb0m">842 ms</td>
    <td class="tg-pb0m">48.383921592881876</td>
    <td class="tg-pb0m">3344 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">28.125</td>
    <td class="tg-7zrl">2013-09-05 20:52:02+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">AlekhandraKhan</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">0.4</td>
    <td class="tg-pb0m">51.84367010674158</td>
    <td class="tg-pb0m">840 ms</td>
    <td class="tg-pb0m">51.84367010674158</td>
    <td class="tg-pb0m">3361 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">28.125</td>
    <td class="tg-7zrl">2013-02-13 12:53:41+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">DennaMcsparren</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">27.411227880361174</td>
    <td class="tg-pb0m">826 ms</td>
    <td class="tg-za14">19.161227880361174</td>
    <td class="tg-pb0m">3109 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-7zrl">2014-03-04 18:11:08+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">YukikoTretter</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">36.87155638370129</td>
    <td class="tg-pb0m">843 ms</td>
    <td class="tg-pb0m">28.62155638370129</td>
    <td class="tg-pb0m">3145 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-7zrl">2014-03-02 10:38:13+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">RochelAmaro</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">38.67176682142858</td>
    <td class="tg-pb0m">854 ms</td>
    <td class="tg-za14">30.421766821428577</td>
    <td class="tg-pb0m">3128 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-7zrl">2014-02-20 22:28:03+00:00</td>
  </tr>
  <tr>
    <td class="tg-pb0m">ElyseKendell</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">en</td>
    <td class="tg-pb0m">4.4</td>
    <td class="tg-pb0m">25.833959055013928</td>
    <td class="tg-pb0m">900 ms</td>
    <td class="tg-pb0m">17.583959055013928</td>
    <td class="tg-pb0m">3133 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-7zrl">2014-02-26 08:57:36+00:00</td>
  </tr>
</tbody>
</table>

Estos resultados los saqué de local, ya que estaba teniendo unos problemas para hacer andar el script que nos mandó Magui, para aprovechar el tiempo. En el dataset en inglés se comportó de la manera esperada.

Los tiempos no mejoraron mucho, es prácticamente 1 segundo menos de lo calculado anteriormente. Dejé la métrica en ms para que se pueda hacer una comparación inmediata con el tiempo anterior.

# Análisis tabla

Se hizo pruebas en la fórmula de cálculo de la credibilidad de usuario, donde se integró la nueva funcionalidad de detección de bots. Se seleccionaron 14 cuentas de Twitter, entre ellas cuentas que utilizan el idioma inglés y otras en español, agrupando 3 cuentas genuinas y 4 cuentas reconocidas como bots para cada lenguaje. Adicionalmente, se incluyó el puntaje obtenido por Botometer para cada cuenta, para realizar el análisis de resultados.

Como se puede ver en la tabla de cuentas en inglés, se hizo un primer cálculo de la credibilidad de las cuentas utilizando el modelo de credibilidad inicial, obteniendo un porcentaje de credibilidad por cada usuario, destacando también el valor de los dos filtros que también componen la credibilidad de usuario "Verified Weight" y "Creation Weight".

Al calcular nuevamente la credibilidad, pero incluyendo el filtro de detección de bots, se puede observar que la credibilidad sigue siendo la misma para las cuentas que son clasificadas como "genuinas", mientras que las cuentas que fueron clasificadas como "bot" han efectivamente bajado su credibilidad, acorde a la fórmula nueva de credibilidad de usuario.

En la tabla de español, se puede observar este mismo patrón de funcionamiento dependiendo de la clasificación realizada por el algoritmo. En este caso, existen dos cuentas de "bot" que no bajan su credibilidad, esto debido a que sus valores para Verified Weight y Creation Weight tenían valor cero respectivamente.

Un patrón que se puede observar dentro de las tablas es que las cuentas que son clasificadas como bot son más recientes en comparación a las clasificadas como genuinas, por ende obteniendo menor puntaje de credibilidad de usuario en el filtro de creación de cuenta. Esto se ve con mayor claridad dentro de la tabla en español, donde dos cuentas bot son tan recientes que no poseen valor de credibilidad respecto a su antigüedad en la plataforma de Twitter. Esta información no es concluyente para poder determinar si una cuenta es bot o no, pero es un patrón observable en las pruebas realizadas.

Adicionalmente, se puede ver una gran diferencia de tiempos de ejecución al comparar el modelo de credibilidad antiguo y el con detección de bots. Esto se debe mayoritariamente a que la funcionalidad de detección se encuentra implementada en Python. La ejecución de este script puede ser más lenta debido a que Python utiliza un intérprete (se ejecuta por una máquina virtual que es emulada por la CPU), en vez de un compilador como otros lenguajes donde se realiza una optimización del código \cite{PythonInterpreter}.

Así mismo, dada la referencia \cite{GuillaumeMarceau}, los programas realizados por lenguajes de programación como Python (scripting), suelen ser más cortos en tamaño, pero más lentos en ejecución, volviéndolos ideales para la creación de prototipos y trabajos no críticos en rendimiento.


@misc{PythonInterpreter,
	title = {500 Lines or Less {$\vert$} A Python Interpreter Written in Python},
	year = {2022},
	month = jun,
	note = {[Online; accessed 24. Sep. 2022]},
	url = {https://www.aosabook.org/en/500L/a-python-interpreter-written-in-python.html}
}

@misc{GuillaumeMarceau,
	title = {The speed, size and dependability of programming languages},
	year = {2022},
	month = aug,
	note = {[Online; accessed 24. Sep. 2022]},
	url = {http://blog.gmarceau.qc.ca/2009/05/speed-size-and-dependability-of.html}
}
"""

#1GisellePizarro

X5=[[0,0,0,0,0,93918,97,87,3388,8,0,0,0,3.843946298,-0.259838782]]
test=model2.predict(X5)
print(test)

#1Nicoleromany

X5=[[0,0,0,1,1,9993,1349,382,4955,20,0.014285714,0,0.014285714,4.730480918,-0.002091181]]
test=model2.predict(X5)
print(test)

#AlekhandraKhan

X5=[[1,2,0,1,0,57014,4136,316,1595,16,0.011494253,0,0,4.385477182,-0.106812661]]
test=model2.predict(X5)
print(test)

#DennaMcsparren

X5=[[0,1,0,0,0,108,116,770,0,0,0,0,0,3.762246434,-0.333849659]]
test=model2.predict(X5)
print(test)

#YukikoTretter

X5=[[0,0,0,0,0,119,27,166,0,0,0,0,0,4.128256101,-0.07127905]]
test=model2.predict(X5)
print(test)

#RochelAmaro

X5=[[2,2,1,0,0,121,41,169,0,1,0,0.007194245,0,4.010537459,-0.172158554]]
test=model2.predict(X5)
print(test)

#ElyseKendell

X5=[[0,0,1,0,0,112,149,928,0,1,0,0.010416667,0,4.442918986,-0.040715409]]
test=model2.predict(X5)
print(test)

"""# Prueba Nueva Implementación Credibilidad con filtro bot

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-pb0m{border-color:inherit;text-align:center;vertical-align:bottom}
.tg .tg-lboi{border-color:inherit;text-align:left;vertical-align:middle}
.tg .tg-za14{border-color:inherit;text-align:left;vertical-align:bottom}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-za14">user</th>
    <th class="tg-za14">label</th>
    <th class="tg-za14">Botometer</th>
    <th class="tg-za14">credibility (old)</th>
    <th class="tg-za14">time (old)</th>
    <th class="tg-za14">credibility(w/ bot detection)</th>
    <th class="tg-za14">time</th>
    <th class="tg-za14">new credibility implementation</th>
    <th class="tg-za14">time</th>
    <th class="tg-za14">Detected as</th>
    <th class="tg-za14">Verified Weight</th>
    <th class="tg-za14">Creation Weight</th>
    <th class="tg-za14">Creation Date</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-za14">@BarackObama</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">1.9</td>
    <td class="tg-za14">91.64567307692309</td>
    <td class="tg-za14">1041 ms</td>
    <td class="tg-za14">59.67692307692308</td>
    <td class="tg-za14">3245 ms</td>
    <td class="tg-za14">86.85036057692308</td>
    <td class="tg-za14">3245 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">46.875</td>
    <td class="tg-za14">2007-03-05 22:08:25+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@NASA</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">2.3</td>
    <td class="tg-za14">92.01061046511629</td>
    <td class="tg-za14">1223 ms</td>
    <td class="tg-za14">92.01061046511629</td>
    <td class="tg-za14">3144 ms</td>
    <td class="tg-za14">92.01061046511629</td>
    <td class="tg-za14">3144 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">46.875</td>
    <td class="tg-za14">2007-12-19 20:20:32+00:00</td>
  </tr>
  <tr>
    <td class="tg-lboi">@YouTube</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">1.6</td>
    <td class="tg-za14">87.49239864864865</td>
    <td class="tg-za14">910 ms</td>
    <td class="tg-za14">87.49239864864865</td>
    <td class="tg-za14">3206 ms</td>
    <td class="tg-za14">87.49239864864865</td>
    <td class="tg-za14">3206 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">34.375</td>
    <td class="tg-za14">2007-11-13 21:43:46+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@nytimes</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">3.4</td>
    <td class="tg-za14">91.99875</td>
    <td class="tg-za14">904 ms</td>
    <td class="tg-za14">91.99875</td>
    <td class="tg-za14">3194 ms</td>
    <td class="tg-za14">91.99875</td>
    <td class="tg-za14">3194 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">46.875</td>
    <td class="tg-za14">2007-03-02 20:41:42+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@elonmusk</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">1.2</td>
    <td class="tg-za14">89.56940789473684</td>
    <td class="tg-za14">930 ms</td>
    <td class="tg-za14">89.56940789473684</td>
    <td class="tg-za14">3199 ms</td>
    <td class="tg-za14">89.56940789473684</td>
    <td class="tg-za14">3199 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">40.625</td>
    <td class="tg-za14">2009-06-02 20:12:29+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@ladygaga</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">0.6</td>
    <td class="tg-za14">80.2575</td>
    <td class="tg-za14">1048 ms</td>
    <td class="tg-za14">49.32</td>
    <td class="tg-za14">3340 ms</td>
    <td class="tg-za14">75.616875</td>
    <td class="tg-za14">3340 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">43.75</td>
    <td class="tg-za14">2008-03-26 22:37:48+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@TheEllenShow</td>
    <td class="tg-pb0m">-</td>
    <td class="tg-pb0m">1.2</td>
    <td class="tg-za14">90.81369047619049</td>
    <td class="tg-za14">857 ms</td>
    <td class="tg-za14">59.87619047619048</td>
    <td class="tg-za14">3231 ms</td>
    <td class="tg-za14">86.17306547619049</td>
    <td class="tg-za14">3231 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">50</td>
    <td class="tg-za14">43.75</td>
    <td class="tg-za14">2008-08-14 03:50:42+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@1GisellePizarro</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">0.6</td>
    <td class="tg-pb0m">36.192919815217394</td>
    <td class="tg-za14">1044 ms</td>
    <td class="tg-za14">36.192919815217394</td>
    <td class="tg-za14">3245 ms</td>
    <td class="tg-za14">36.145901601351355</td>
    <td class="tg-za14">3245 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">34.375</td>
    <td class="tg-za14">2011-02-22 04:37:34+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@1Nicoleromany</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">0.2</td>
    <td class="tg-pb0m">48.38181343203563</td>
    <td class="tg-za14">842 ms</td>
    <td class="tg-za14">48.383921592881876</td>
    <td class="tg-za14">3202 ms</td>
    <td class="tg-za14">48.38181343203563</td>
    <td class="tg-za14">3202 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">28.125</td>
    <td class="tg-za14">2013-09-05 20:52:02+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@AlekhandraKhan</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-pb0m">0.4</td>
    <td class="tg-pb0m">51.84367010674158</td>
    <td class="tg-za14">840 ms</td>
    <td class="tg-za14">51.84367010674158</td>
    <td class="tg-za14">3193 ms</td>
    <td class="tg-za14">51.84339849747134</td>
    <td class="tg-za14">3193 ms</td>
    <td class="tg-pb0m">human</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">28.125</td>
    <td class="tg-za14">2013-02-13 12:53:41+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@DennaMcsparren</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">27.411227880361174</td>
    <td class="tg-za14">826 ms</td>
    <td class="tg-za14">19.161227880361174</td>
    <td class="tg-za14">3146 ms</td>
    <td class="tg-za14">19.16366886440678</td>
    <td class="tg-za14">3146 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-za14">2014-03-04 18:11:08+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@YukikoTretter</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">36.87155638370129</td>
    <td class="tg-za14">843 ms</td>
    <td class="tg-za14">28.62155638370129</td>
    <td class="tg-za14">3096 ms</td>
    <td class="tg-za14">28.62155638370129</td>
    <td class="tg-za14">3096 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-za14">2014-03-02 10:38:13+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@RochelAmaro</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">5</td>
    <td class="tg-pb0m">38.67176682142858</td>
    <td class="tg-za14">854 ms</td>
    <td class="tg-za14">30.421766821428577</td>
    <td class="tg-za14">3287 ms</td>
    <td class="tg-za14">30.421766821428577</td>
    <td class="tg-za14">3287 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-za14">2014-02-20 22:28:03+00:00</td>
  </tr>
  <tr>
    <td class="tg-za14">@ElyseKendell</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-pb0m">4.4</td>
    <td class="tg-pb0m">25.833959055013928</td>
    <td class="tg-za14">900 ms</td>
    <td class="tg-za14">17.583959055013928</td>
    <td class="tg-za14">3130 ms</td>
    <td class="tg-za14">17.583959055013928</td>
    <td class="tg-za14">3130 ms</td>
    <td class="tg-pb0m">bot</td>
    <td class="tg-za14">0</td>
    <td class="tg-za14">25</td>
    <td class="tg-za14">2014-02-26 08:57:36+00:00</td>
  </tr>
</tbody>
</table>

Esta son las cuentas con las que haré las pruebas una vez deje implementado la nueva forma de penalización al ser bots. Estaré trabajando implementando esto durante el fin de semana, a más tardar espero poder tener los resultados de esta prueba el martes en la tarde. Las cuentas de prueba en español las dejaré en el otro notebook.

# Investigación de Cuentas sin etiqueta EN

@BarackObama -> human

@NASA -> bot/Híbrido

@YouTube -> human/híbrido

@nytimes -> bot/Híbrido

@elonmusk -> human

@ladygaga -> human

@TheEllenShow -> bot/híbrido


Estoy viendo como justificar estas etiquetas acorde a patrones o comportamiento que he visto.
"""

#BarackObama

X5=[[4663,16729,0,1,0,16606,133411531,580774,0,221175,0.003831418,0,0,4.525259984,0.057247354]]
test=model2.predict(X5)
print(test)

#NASA

X5=[[5728,28527,0,2,0,68713,63375013,184,14285,99732,0.006688963,0,0,4.74673207,0.045830986]]
test=model2.predict(X5)
print(test)

#YouTube

X5=[[693,4513,0,2,0,47225,76459785,1205,6082,80174,0.00729927,0,0,4.782939263,-0.00625625]]
test=model2.predict(X5)
print(test)

#nytimes

X5=[[680,2480,0,2,0,485882,54411858,879,18965,217482,0.006734007,0,0,4.647599185,0.121807918]]
test=model2.predict(X5)
print(test)

#elonmusk

X5=[[815,11381,0,0,0,19384,107429082,122,14265,98066,0,0,0,4.206841309,0.032058425]]
test=model2.predict(X5)
print(test)

#ladygaga

X5=[[892,7782,0,1,2,9954,84944026,117470,3010,202586,0.005154639,0,0.010309278,4.773081952,0.045283129]]
test=model2.predict(X5)
print(test)

#TheEllenShow

X5=[[74,454,1,1,1,24194,77308205,26229,4635,95669,0.006993007,0.006993007,0.006993007,4.580688796,0.046562912]]
test=model2.predict(X5)
print(test)

filename = 'RandomForestClassifier_ENG-Colab.sav'
pickle.dump(model2, open(filename, 'wb'))